{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/a-forty-two/EY_batch11_3Mar/blob/main/5Mar_001_llm_intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "PaSZyBhOL3GR"
      },
      "source": [
        "# <font color=\"#76b900\">**Notebook 1:** Getting Started With Large Language Models</font>\n",
        "\n",
        "**Welcome To The Course!** This is the first content notebook and is intended to springboard you into the LLM loading workflow with some insights about our problem, our resources, and our objectives!\n",
        "\n",
        "#### **Learning Objectives:**\n",
        "\n",
        "- Review some basic assumptions about deep learning and show how they extend to language modeling.\n",
        "- Pull in your first LLM into the environment, investigate its architecture, and see how it performs!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFMdu21rL3GS"
      },
      "source": [
        "-------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2Dhs90vKb9d"
      },
      "source": [
        "<br>\n",
        "\n",
        "## **Part 1.1:** Recalling Deep Learning\n",
        "\n",
        "Throughout your learning adventure with deep learning, you have probably optimized a variety of models for tasks like classification and regression. In order, you probably advanced in something like the following:\n",
        "\n",
        "- When you started out, you used **linear and logistic regression** to model and interpret simple linear relationships that associated your inputs with your outputs.\n",
        "- When that wasn't enough, you started **stacking linear layers one after another and adding non-linear activations** to give your model more predictive power.\n",
        "- When your data started getting intractably high-dimensional, you started using more **informed sparsely-connected techniques like convolution** to add more control to your reasoning criteria.\n",
        "- When you realized that you didn't have enough data to properly train your models for each specific task, you got **pre-trained components (i.e. VGG-16/ResNet)** that were trained on a giant repository of training data and already contained the necessary logic you wanted.\n",
        "\n",
        "> <div><img src=\"imgs/machine-learning-process.jpg\" width=\"800\"/></div>\n",
        ">\n",
        "> **Source: [High-Performance Data Science with RAPIDS | NVIDIA](https://www.nvidia.com/en-us/deep-learning-ai/software/rapids/)**\n",
        "\n",
        "If you’ve gone through these steps, you already possess the foundational skills to tackle complex topics, including the vast field of language modeling.\n",
        "\n",
        "Like vision, language is highly complex and high-dimensional. For instance, a simple 200x200 color image contains $200\\times 200\\times 3 = 120,000$ features! Now, consider the even larger number of combinations possible in natural language—**it’s enormous!** Fortunately, creative techniques and large pre-trained models simplify the problem, providing efficient solutions.\n",
        "\n",
        "**This course will show you how to approach language modeling, the tools available, and the types of problems you can solve!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tkyrae_YL3GS"
      },
      "source": [
        "-------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3DfWbMaOQxe"
      },
      "source": [
        "<br>\n",
        "\n",
        "## **Part 1.2:** Pulling In Our First LLM\n",
        "\n",
        "Rather than building models from scratch, this course focuses on using powerful tools to simplify the process. A great place to start is **HuggingFace** &#x1F917;.\n",
        "\n",
        "[**HuggingFace**](https://huggingface.co/) is an open-source community that offers simple strategies for accessing, developing, and hosting large deep learning models for testing and deployment. The topics they support span many tasks and modalities, but we'll be focusing on large language models (**LLMs**) for most of this course.\n",
        "\n",
        "When searching through the [**HuggingFace Models catalog**](https://huggingface.co/models?sort=downloads&search=bert), you'll quickly stumble upon the [`bert-base-uncased`](https://huggingface.co/bert-base-uncased) model. Taking a look at its card, you'll see several interesting things:\n",
        "\n",
        "- **The [`transformers`](https://github.com/huggingface/transformers) Package**: Loading in the model requires the use of the [`transformers`](https://github.com/huggingface/transformers) package, which is HuggingFace’s primary library for language models. Its name refers to the transformer architecture, which we’ll explore further in upcoming sections. You’ll be using `transformers` throughout this course, so feel free to dive into the source code as you go.\n",
        "\n",
        "- **Pipelines**: HuggingFace simplifies complex deep learning tasks with its [Pipeline API](https://huggingface.co/docs/transformers/main_classes/pipelines). A pipeline allows you to move from input to output without worrying about the internal workings. We’ll demonstrate this with the `bert-base-uncased` model for mask-filling.\n",
        "\n",
        "As a representative example, we can go ahead and pull in the discussed [`bert-base-uncased`](https://huggingface.co/bert-base-uncased) model and test it out:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ERP_WrTRP2N",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "## Loading in the pipeline and predict the mask fill (example from model card)\n",
        "unmasker = pipeline(\n",
        "    'fill-mask',\n",
        "    model='bert-base-uncased',\n",
        "    device='cuda',  ## Feel free to use GPU. For such a small model, not necessary\n",
        ")\n",
        "unmasker(\"Hello I'm a [MASK] model.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfUHbutJO4Nr"
      },
      "source": [
        "**And just like that, it works!** You have just obtained an intuitive answer that makes sense with human logic, which makes you almost forget that there is actually a deep learning model calculating probabilities to make this all work.\n",
        "\n",
        "**But that's what this course is for: to see what's actually going on behind the scenes and know how to use it to make interesting and useful systems.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qA9vUG_Ny9Cn"
      },
      "source": [
        "<hr>\n",
        "<br>\n",
        "\n",
        "## **Part 1.3:** Dissecting The Pipeline\n",
        "\n",
        "While the pipeline provides a clean interface—taking strings in and returning a dictionary—it doesn’t give us much insight into what’s happening under the hood. Let’s peel back a layer and examine the approximate inner workings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atoVpxD7zgFO",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, BertTokenizer, BertModel, FillMaskPipeline, AutoModelForMaskedLM, BertForMaskedLM, BertForPreTraining\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel        ## General-purpose fully-automatic\n",
        "from transformers import AutoModelForMaskedLM            ## Default import for FillMaskPipeline\n",
        "from transformers import BertTokenizer, BertForMaskedLM  ## Realized components after automatic resolution\n",
        "\n",
        "class MyMlmPipeline(FillMaskPipeline):\n",
        "    ## My Masked Language Modeling Pipeline\n",
        "\n",
        "    ### CASE 0: Construct your pipeline automatically by pulling in the components\n",
        "    ###   with their respective configs from HuggingFace. Pipeline assumes preprocessing/postprocessing.\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        ## The fully-automatic version\n",
        "        super().__init__(\n",
        "            tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased'),\n",
        "            model = AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\"),\n",
        "            # model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\"),\n",
        "            *args, **kwargs  ## <- pass in any extra arguments\n",
        "        )\n",
        "\n",
        "    ### CASE 1: Uncomment out the __call__ method to see what data is flowing.\n",
        "    \"\"\"\n",
        "    def __call__(self, string, verbose=True):\n",
        "        ## Verbose argument just there for our convenience\n",
        "        input_tensors = self.preprocess(string)\n",
        "        if verbose: print('\\npreprocess outputs:\\n', input_tensors, '\\n')\n",
        "        output_tensors = self.forward(input_tensors)\n",
        "        if verbose: print('forward outputs:\\n', output_tensors, '\\n')\n",
        "        output = self.postprocess(output_tensors)\n",
        "        return output\n",
        "    \"\"\"\n",
        "\n",
        "    ### CASE 2: Uncomment out the manual overrides below to verify the pipeline still works\n",
        "    \"\"\"\n",
        "    def preprocess(self, string):\n",
        "        string = [string] if isinstance(string, str) else string\n",
        "        inputs = self.tokenizer(string, return_tensors=\"pt\")           ### strings -> indices\n",
        "        inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}          ### move to GPU\n",
        "        return inputs\n",
        "\n",
        "    def forward(self, tensor_dict):\n",
        "        output_tensors = self.model.forward(**tensor_dict)             ### indices -> vectors -> probabilities\n",
        "        return {**output_tensors, **tensor_dict}\n",
        "\n",
        "    def postprocess(self, tensor_dict):\n",
        "        ## Very Task-specific; see FillMaskPipeline.postprocess\n",
        "        tensor_dict = {k: v.to(\"cpu\") for k, v in tensor_dict.items()} ### move off GPU\n",
        "        return super().postprocess(tensor_dict)                        ### probabilities (or vectors) -> outputs\n",
        "    \"\"\"\n",
        "\n",
        "unmasker = MyMlmPipeline(device=\"cuda\")\n",
        "unmasker(\"Hello, Mr. Bert! How is it [MASK]?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLoEmptNztUb"
      },
      "source": [
        "We can also see that the model primarily consists of two core components:\n",
        "- **Tokenizer**: Converts input strings into a format the model can process.\n",
        "- **Model**: The deep learning engine that transforms input tensors into output tensors.\n",
        "\n",
        "Together, these components support the pipeline’s intuitive workflow:\n",
        "- `preprocess`: human-intuitive input $\\to$ tensor inputs. Facilitated by `tokenizer`\n",
        "- `forward`: tensor inputs $\\to$ tensor outputs. Facilitated by `model`\n",
        "- `postprocess`: tensor outputs $\\to$ human-intuitive outputs. Facilitated by the pipeline task.\n",
        "\n",
        "For deep learning, this flow makes sense. The model operates on numbers, but the users are working with words, to this abstraction greatly simplifies the LLM interface for typical scenarios."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLzpMuxnL3GV"
      },
      "source": [
        "-------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3elKb_tL3GV"
      },
      "source": [
        "<br>\n",
        "\n",
        "## **Part 1.4:** Your Course Environment\n",
        "\n",
        "Since these models are so easy to pull in, we need to establish some limitations! Throughout this course, feel free to explore different models, but be mindful of their size—**your compute environment is powerful, but not limitless.** We’ve pre-loaded several models for you, which you can review (along with their licenses) in the [**extras_and_licenses/99_licenses.ipynb**](extras_and_licenses/99_licenses.ipynb).\n",
        "\n",
        "For this course, you’ll be using a high-performance compute setup which is sufficient for the exercises and has the following key resources:\n",
        "\n",
        "- **System Memory**: Large language models can require massive memory, with some needing tens or even hundreds of GB. This environment is equipped to handle significant workloads, but try to keep model sizes reasonable unless otherwise instructed.\n",
        "  \n",
        "- **GPU**: GPU acceleration is crucial for fast deep learning, especially given the parallel processing demands of deep models. The thousands of [**CUDA cores**](https://en.wikipedia.org/wiki/CUDA) in modern NVIDIA GPUs significantly speed up training and inference by performing parallelizable tasks (like high-dimensional matrix multiplication) very quickly.\n",
        "  \n",
        "    - **GPU RAM**: Large models need to reside in GPU memory for fast inference. Limited GPU RAM can be a bottleneck for using accelerated LLMs, so manage your model sizes accordingly.\n",
        "\n",
        "Here’s a breakdown of your allocated resources:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "tv15wOcgL3GV"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "echo \"\"\"\n",
        "===================================================\n",
        "GPU SPECIFICATION\n",
        "===================================================\"\"\"\n",
        "nvidia-smi\n",
        "echo \"\"\"\n",
        "===================================================\n",
        "MEMORY SPECIFICATION\n",
        "===================================================\"\"\"\n",
        "free -h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NV8KvX1IL3GV"
      },
      "source": [
        "<br>\n",
        "\n",
        "**So, while you have a solid compute budget, it’s not infinite!**\n",
        "\n",
        "Before moving to the next notebook, restart the Jupyter kernel using the following code to clear memory and avoid issues with future notebooks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "_MQzjkMXL3GV"
      },
      "outputs": [],
      "source": [
        "import IPython\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
